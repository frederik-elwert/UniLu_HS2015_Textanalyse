{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Notebook enthält eine kommentierte Version der Datei *bundesscraper.py*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import aller benötigten Module und Einstellen der Sprache für die Erkennung von Datumsangaben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "# Required for parsing German dates\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8')\n",
    "\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist gute Praxis, beim Crawlen einer Seite kleine Pausen einzulegen, damit der Server nicht zu stark beeinträchtigt wird. Das Interval wird hier auf zwei Sekunden festgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAUSE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein typischer Fall beim Crawlen ist, dass es zwei Typen von Seiten gibt: Eine **Übersichtsseite**, die eine Liste von Suchtreffern, Artikeln, o.ä. enthält, und **Detailseiten**, die zu jedem einzelnen dieser Objekte ausführliche Angaben liefern. Eine Suche in einem Online-Shop wird z.B. eine Liste von passenden Artikeln aufführen, während ein Klick auf einen einzelnen Artikel zu einer Seite mit weiteren Angaben, Bildern und Bewertungen führt.\n",
    "\n",
    "Oft ist die Übersichtsseite in der Praxis dabei auf mehrere Seiten aufgeteilt, die jeweils eine bestimmte Anzahl von Treffern enthalten. Über eine Blätterfunktion kann man dann weitere Treffer aufrufen, etwa:\n",
    "\n",
    "| 1 | 2 | 3 | 4 | nächste Seite |\n",
    "\n",
    "Die Hauptfunktion ist `crawl()`. Sie nimmt zwei Parameter entgegen: `start_url` ist die erste Übersichtsseite, `outfile` ist der Name der Datei, in der das Ergebnis gespeichert werden soll. Ein typischer Aufruf sieht also so aus:\n",
    "\n",
    "```python\n",
    "crawl('http://www.bundesregierung.de/SiteGlobals/Forms/Webs/Breg/Suche/DE/Nachrichten/Redensuche2_formular.html',\n",
    "      'reden.csv')\n",
    "```\n",
    "\n",
    "Die Funktion ruft dabei weitere Funktionen auf. Die Grundlogik ist dabei:\n",
    "\n",
    "1. Alle Links zu Detailseiten finden: `find_links()`\n",
    "2. Für jeden dieser Links die Detailseite aufrufen und die relevanten Daten extrahieren: `scrape_page()`\n",
    "3. Das Ergebnis in eine CSV-Datei speichern: `save()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl(start_url, outfile):\n",
    "    \"\"\"\n",
    "    Crawl a list page, extract details and save results as CSV.\n",
    "    \n",
    "    \"\"\"\n",
    "    links = find_links(start_url)\n",
    "    pages = [scrape_page(link) for link in links]\n",
    "    save(pages, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `find_links()` dient dazu, alle Übersichtsseiten durchzugehen und die jeweiligen Links zu den Detailseiten in einer gemeinsamen Liste zusammenzuführen. Dabei guckt die Funktion für jede Übersichtsseite, ob sie einen Link auf eine weitere Übersichtsseite enthält. Falls ja, wird diese weitere Seite ebenfalls abgerufen, andernfalls ist die Liste vollständig und wird zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_links(start_url):\n",
    "    \"\"\"\n",
    "    Find links to details pages from all subsequent list pages.\n",
    "    \n",
    "    \"\"\"\n",
    "    links = []\n",
    "    next_page = start_url\n",
    "    while next_page:\n",
    "        new_links, next_page = scrape_list_page(next_page)\n",
    "        links.extend(new_links)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `scrape_list_page()` ist dafür zuständig, eine konkrete Übersichtsseite abzurufen. Sie sucht dabei nach zwei Elementen, die sie anschließend zurückgibt:\n",
    "\n",
    "1. Die Links zu den Detailseiten, und\n",
    "2. Den Link zur nächsten Übersichtsseite. Gibt es keinen solchen Link, wird `None` zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_list_page(url):\n",
    "    \"\"\"\n",
    "    Extract links to details pages from a specific list page.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Wait\n",
    "    time.sleep(PAUSE)\n",
    "    # Get and parse page\n",
    "    doc = html.parse(url)\n",
    "    root = doc.getroot()\n",
    "    root.make_links_absolute(url)\n",
    "    # Extract links to details pages\n",
    "    link_elements = root.cssselect('#searchResults h3 a')\n",
    "    links = [element.get('href') for element in link_elements]\n",
    "    # Extract link to next page\n",
    "    next_link_elements = root.cssselect('.forward a')\n",
    "    if next_link_elements:\n",
    "        next_page = next_link_elements[0].get('href')\n",
    "    else:\n",
    "        next_page = None\n",
    "    # Return\n",
    "    return (links, next_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `scrape_page()` durchsucht eine Detailseite und extrahiert die relevanten Informationen. Für jede Seite wird dabei nach diesen Angaben gesucht, die als Liste zurückgegeben werden: URL, Titel, Datum, Ort, Abtract und Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    \"\"\"\n",
    "    Scrape information from a details page.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Wait\n",
    "    time.sleep(PAUSE)\n",
    "    # Get and parse page\n",
    "    doc = html.parse(url)\n",
    "    root = doc.getroot()\n",
    "    # Title\n",
    "    title = root.cssselect('#main h1')[0].text_content()\n",
    "    # Metadata fields: Date and place\n",
    "    date = None\n",
    "    place = None\n",
    "    metadata = root.cssselect('#main dl')\n",
    "    if metadata:\n",
    "        metadata = metadata[0]\n",
    "        keys = metadata.cssselect('dt')\n",
    "        values = metadata.cssselect('dd')\n",
    "        for key, value in zip(keys, values):\n",
    "            if key.text_content().strip(':') == 'Datum':\n",
    "                date = parse_date(value.text_content())\n",
    "            elif key.text_content().strip(':') == 'Ort':\n",
    "                place = value.text_content()\n",
    "    # Abstract and text\n",
    "    abstract = root.cssselect('#main .abstract')\n",
    "    if abstract:\n",
    "        abstract = abstract[0].text_content()\n",
    "    else:\n",
    "        abstract = None\n",
    "    paragraphs = root.cssselect('#main .basepage_pages > p')\n",
    "    paragraphs = [p.text_content() for p in paragraphs]\n",
    "    text = '\\n\\n'.join(paragraphs)\n",
    "    # Strip superfluous session ID\n",
    "    if ';jsessionid' in url:\n",
    "        url = url.split(';jsessionid')[0]\n",
    "    # Return\n",
    "    return [url, title, date, place, abstract, text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `parse_date()` ist der Übersichtlichkeit halber aus der Funktion `scrape_page()` ausgelagert. Sie dient dazu, Datumsangaben in ein standardisiertes Format zu überführen. Dabei werden zwei Formen von Datumsangaben erkannt: *2. November 2014* und *2.11.2014*. Beide Formen werden in der Ergebnisdatei als *2014-11-02* gespeichert.\n",
    "\n",
    "Beide Datumsvarianten sind in der Variablen `date_patterns` in zwei Formen gespeichert: Einmal als regulärer Ausdruck, und einmal als dazugehöriges Datumsmuster. Mit dem regulären Ausdruck wird überprüft, ob das Datum im Text in der jeweiligen Form vorliegt. Falls ja, wird das entsprechende Datumsmuster zum Parsen verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_date(date_string):\n",
    "    \"\"\"\n",
    "    Parse dates from a string and return a `datetime.date()` object.\n",
    "    \n",
    "    \"\"\"\n",
    "    date_string = date_string.strip()\n",
    "    date_patterns = [\n",
    "        (r'\\d+\\.\\d+\\.\\d+', '%d.%m.%Y'),\n",
    "        (r'\\d+\\. \\w+ \\d+', '%d. %B %Y')\n",
    "    ]\n",
    "    date = None\n",
    "    for regex, pattern in date_patterns:\n",
    "        match = re.match(regex, date_string)\n",
    "        if match:\n",
    "            # Limit date string to part that matches the regex.\n",
    "            # Prevents parsing errors with additional data like \" 17:00 Uhr\"\n",
    "            date_string = match.group(0)\n",
    "            date = datetime.datetime.strptime(date_string, pattern).date()\n",
    "    return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `save()` speichert das Ergebnis als CSV-Datei. Der Parameter `pages` ist dabei eine Liste von Seiten, wobei jede Seite wieder eine Liste von URL, Titel, etc. ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(pages, outfile):\n",
    "    \"\"\"\n",
    "    Save list of pages to a CSV file.\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(outfile, 'w', newline='') as csvfile:   \n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['url', 'title', 'date', 'place', 'abstract', 'text'])\n",
    "        for page in pages:\n",
    "            writer.writerow(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bislang wurden nur die einzelnen Funktionen definiert. Um nun zu crawlen, wird die Hauptfunktion aufgerufen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START_URL = 'http://www.bundesregierung.de/SiteGlobals/Forms/Webs/Breg/Suche/DE/Nachrichten/Redensuche2_formular.html?nn=8988&searchtype.HASH=e5e7611fd92022248dd0&path.HASH=631e69fa04dc338f202c&path=%2Fbpainternet%2Fcontent%2Fde%2Frede*+%2Fbpainternet%2Fcontentarchiv%2Fde%2Farchiv17%2Frede*&doctype=speech&doctype.HASH=fb7e3f87bcd598d5d3d&searchtype=news'\n",
    "OUTFILE = 'reden.csv'\n",
    "crawl(START_URL, OUTFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Durchgang kann dabei sehr lange dauern. Die Datei *bundesscraper.py* gibt daher regelmäßig Zwischeninformationen aus, um das Arbeiten des Programms nachvollziehen zu können."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
