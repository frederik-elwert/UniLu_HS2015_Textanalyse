{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text als Sprache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als *Natural Language Processing* bezeichnet man die computergestützte Verarbeitung von natürlichen Sprachen, also von von Menschen gesprochenen Sprachen im Gegensatz zu Programmiersprachen. Computer sind gut darin, Programmiersprachen mit ihren starren Regeln zu verstehen, aber menschliche Sprachen mit ihren Unregelmäßigkeiten stellen immer noch eine Herausforderung dar. Die Wissenschaft, die sich mit diesen Problemen beschäftigt, ist die Computerlinguistik.\n",
    "\n",
    "Methoden der Computerlinguistik sind mittlerweile für eine Vielzahl von Anwendungsbereichen zentral. So sind etwa Suchmaschinen darauf trainiert, nicht nur den exakten Text einer Suchanfrage zu finden, sondern auch andere Flexionsformen. Eine Google-Suche nach „Parties in Hamburg“ findet so z.B. auch die Seite [Nachtleben & Party – hamburg.de](http://www.hamburg.de/nachtleben-party/). Dazu muss die Suchmaschine wissen, dass »Party« der Singular von »Parties« ist, und dass »in« als Präposition für den gesuchten Inhalt nicht zwingend relevant ist.\n",
    "\n",
    "Für die Analyse von Texten sind insbesondere zwei Verfahren interessant: *Generalisierung* und *Selektion*.\n",
    "\n",
    "**Generalisierung**\n",
    "\n",
    "Mit Generalisierung ist die Abbildung von verschiedenen Varianten auf eine gemeinsame, abstraktere Form gemeint. Im obigen Beispiel als die Abbildung der beiden unterschiedlichen Wortformen »Party« und »Parties« auf eine gemeinsame Grundform »Party«. Ziel der Generalisierung in der Textverarbeitung ist es, semantisch gleiche (oder zumindest sehr ähnliche) Einheiten auch dann zu identifizieren, wenn sie in unterschiedlichen Formen auftauchen.\n",
    "\n",
    "Dies kann nicht nur Wortformen betreffen, sondern z.B. auch die Bezeichnungen von Personen. In einem Text kann etwa wechselnd von »Angela Merkel«, »Frau Merkel«, »Bundeskanzlerin Merkel« oder auch nur »die Bundeskanzlerin« die Rede sein.\n",
    "\n",
    "**Selektion**\n",
    "\n",
    "Mit Selektion ist gemeint, für die Analyse relevante Informationen aus der Gesamtmenge an Informationen zu extrahieren. Im obigen Beispiel wäre dies, »in« für die Suche zu ignorieren. Je nach Analsestrategie kann es sehr unterschiedlich sein, welche Informationen relevant sind und welche nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Ausgangspunkt für NLP ist dabei fast immer der reine Text, ohne Formatierung oder ähnliches. Als Beispiel soll der Anfang einer Rede der deutschen Bundeskanzlerin Angela Merkel dienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lieber Herr Kock,\n",
      "liebe Kollegin, Frau Wanka,\n",
      "meine Damen und Herren,\n",
      "aber besonders: liebe Preisträgerinnen und Preisträger von „Jugend forscht“,\n",
      "\n",
      "ich heiße Sie alle ganz herzlich im Bundeskanzleramt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "filepath = os.path.join('..', 'Daten', 'Rede_Jugend_forscht.txt')\n",
    "\n",
    "with open(filepath) as infile:\n",
    "    rede = infile.read()\n",
    "print(rede[0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir mit Texten arbeiten, sind wir meist aber nicht am Text als Ganzem interessiert, sondern an den Wörtern, aus denen er besteht. Wörter sind in den europäischen Sprachen meist durch Leerzeichen getrennt. So sehen die ersten Wörter nach der Anrede aus aus, wenn man den Text entsprechend aufspaltet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich -- heiße -- Sie -- alle -- ganz -- herzlich -- im -- Bundeskanzleramt -- willkommen. -- Dieses -- Jahr -- ist -- ein -- ganz -- besonderes -- Jahr:\n"
     ]
    }
   ],
   "source": [
    "sample = rede[148:254]\n",
    "\n",
    "print(' -- '.join(sample.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sieht man schon einige Probleme. Insbesondere werden hier die Satzzeichen noch beibehalten. Die verfälschen aber das Ergebnis, denn das Wort heißt ja nicht „willkommen**.**“, sondern einfach „willkommen“. Schon bei so einfachen Aufgaben wie dem Aufspalten kann es also nützlich sein, etwas ausgefeiltere Werkzeuge heranzuziehen. Einen einfachen Einstieg bietet dabei das Modul `TextBlob`, für das es auch eine auf die deutsche Sprache ausgelegte Version gibt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob_de import TextBlobDE as TextBlob\n",
    "blob = TextBlob(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich -- heiße -- Sie -- alle -- ganz -- herzlich -- im -- Bundeskanzleramt -- willkommen -- Dieses -- Jahr -- ist -- ein -- ganz -- besonderes -- Jahr\n"
     ]
    }
   ],
   "source": [
    "print(' -- '.join(blob.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird der Text mit einem geeigneteren Algorithmus, der auch Satzzeichen berücksichtigt, in Wörter aufgespalten.\n",
    "\n",
    "`TextBlob` stellt noch eine Reihe weiterer Methoden aus der Sprachverarbeitung bereit. So kann man etwa Worte auf ihre Grundformen zurückführen. Gerade bei stark flektierenden Sprachen wie dem Deutschen ist das oft nützlich. Diesen Schritt nennt man in der Computerlinguistik Lemmatisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich -- heeissen -- Sie -- all -- ganz -- herzlich -- im -- Bundeskanzleramt -- willkomm -- Dies -- Jahr -- sein -- ein -- ganz -- besonder -- Jahr\n"
     ]
    }
   ],
   "source": [
    "base_forms = blob.words.lemmatize()\n",
    "\n",
    "print(' -- '.join(base_forms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kann sehen, dass der Schritt vergleichsweise fehleranfällig ist. Relevant für die Analyse, wie häufig ein bestimmtes Wort (und nicht eine bestimmte, flektierte Wortform) auftaucht, ist aber, dass eine gemeinsame Form gefunden wird, etwa »all« für »alle, alles, allen …« oder »sein« für »ist, sind, waren, …«.\n",
    "\n",
    "Insgesamt ist der Lemmatisierungsalgorithmus von `TextBlob` im vergleich zu anderen Werkzeugen eher schwach. Zur Demonstration und aufgrund der einfachen Handhabbarkeit reicht er hier aus. In Anwendungsfällen, in denen eine hohe Qualität der Verarbeitung wichtig ist, muss man aber ggf. nach besseren Werkzeugen suchen.\n",
    "\n",
    "Für weitere Analysen kann man auch die Grammatik eines Texts analysieren. Ein grundlegender Schritt ist dabei die Bestimming der Wortart (Substantiv, Verb, etc.), englisch »Part of Speech«. Hierfür werden in der Linguistik meist bestimmte Kürzel verwendet, die von `TextBlob` sind auf [dieser Seite](http://www.clips.ua.ac.be/pages/mbsp-tags) aufgelistet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich/PRP -- heiße/VB -- Sie/PRP -- alle/DT -- ganz/RB -- herzlich/JJ -- im/IN -- Bundeskanzleramt/NN -- willkommen/JJ -- Dieses/DT -- Jahr/NN -- ist/VB -- ein/DT -- ganz/RB -- besonderes/JJ -- Jahr/NN\n"
     ]
    }
   ],
   "source": [
    "print(' -- '.join(['{}/{}'.format(word, tag) for word, tag in blob.pos_tags]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies kann etwa dazu verwendet werden, für bestimmte Anwendungen nur bestimmte Wortarten zu berücksichtigen. Falls etwa nur die Substantive interessieren, lassen sie sich aufgrund der PoS-Information herausfiltern.\n",
    "\n",
    "Um Informationen wie Lemma und Wortart nicht einzeln erzeugen zu müssen, kann man diese mit der `parse()`-Funktion auch in einem Durchlauf erzeugen. Da der Standard-Parser keine Lemmata erzeugt, müssen diese zunächst explizit aktiviert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ich/PRP/B-NP/O/ich heiße/VB/B-VP/O/heeißen Sie/PRP/B-NP/O/sie alle/DT/O/O/all ganz/RB/B-ADJP/O/ganz herzlich/JJ/I-ADJP/O/herzlich im/IN/B-PP/B-PNP/im Bundeskanzleramt/NN/B-NP/I-PNP/bundeskanzleramt willkommen/JJ/B-ADJP/O/willkomm ././O/O/. Dieses/DT/B-NP/O/dies Jahr/NN/I-NP/O/jahr ist/VB/B-VP/O/sein ein/DT/B-NP/O/ein ganz/RB/I-NP/O/ganz besonderes/JJ/I-NP/O/besonder Jahr/NN/I-NP/O/jahr :/:/O/O/:'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob_de import PatternParser\n",
    "lemma_parser = PatternParser(lemmata=True)\n",
    "blob = TextBlob(sample, parser=lemma_parser)\n",
    "parse = blob.parse()\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Form ist auf Anhieb nicht sehr gut lesbar. Es gibt auch eine tabellarische Darstellung, die mit dem Parameter `pprint` aktiviert werden kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            WORD   TAG    CHUNK    ROLE   ID     PNP    LEMMA              \n",
      "                                                                           \n",
      "             ich   PRP    NP       -      -      -      ich                \n",
      "           heiße   VB     VP       -      -      -      heeißen            \n",
      "             Sie   PRP    NP       -      -      -      sie                \n",
      "            alle   DT     -        -      -      -      all                \n",
      "            ganz   RB     ADJP     -      -      -      ganz               \n",
      "        herzlich   JJ     ADJP ^   -      -      -      herzlich           \n",
      "              im   IN     PP       -      -      PNP    im                 \n",
      "Bundeskanzleramt   NN     NP       -      -      PNP    bundeskanzleramt   \n",
      "      willkommen   JJ     ADJP     -      -      -      willkomm           \n",
      "               .   .      -        -      -      -      .                  \n",
      "          Dieses   DT     NP       -      -      -      dies               \n",
      "            Jahr   NN     NP ^     -      -      -      jahr               \n",
      "             ist   VB     VP       -      -      -      sein               \n",
      "             ein   DT     NP       -      -      -      ein                \n",
      "            ganz   RB     NP ^     -      -      -      ganz               \n",
      "      besonderes   JJ     NP ^     -      -      -      besonder           \n",
      "            Jahr   NN     NP ^     -      -      -      jahr               \n",
      "               :   :      -        -      -      -      :                  \n"
     ]
    }
   ],
   "source": [
    "blob2 = TextBlob(sample, parser=PatternParser(lemmata=True, pprint=True))\n",
    "blob2.parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Weiterverarbeitung ist es hilfreich, für jedes Wort auf Informationen wie Wortart, Lemma etc. zugreifen zu können. Dafür kann die erste Form relativ leicht zerlegt werden. Dort werden alle Informationen zu einem Wort durch Schrägstriche getrennt angegeben. Dies ist eine übliche Konvention in der Linguistik. Die Bedeutung der einzelnen Elemente kann man sich anzeigen lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word', 'part-of-speech', 'chunk', 'preposition', 'lemma']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Informationen für jedes Wort (*token*) leicht zugänglich zu machen, kann man in Python die Klasse `namedtuple()` nutzen, mit der man einzelne Felder in einer Liste (nichts anderes ist ein sogenanntes „tuple“ im Grunde) benennen kann. Die Idee ist hierbei, statt `token[4]` für das fünfte Tag zu einem Wort einfach `token.lemma` schreiben zu können.\n",
    "\n",
    "Da die Namen von Eigenschaften in Python kein '-' enthalten dürfen, wird es zunächst durch einen Unterstrich ersetzt. Auf der Grundlage können wir eine neue Datenstruktur (Klasse) erzeugen, die einen einfachen Zugriff auf die einzelnen Informationen erlaubt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Token(word='vor', part_of_speech='IN', chunk='B-PP', preposition='B-PNP', lemma='vor')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "fieldnames = ['word', 'part_of_speech', 'chunk', 'preposition', 'lemma']\n",
    "Token = namedtuple('Token', fieldnames)\n",
    "\n",
    "# Beispiel: „vor“\n",
    "Token('vor', 'IN', 'B-PP', 'B-PNP', 'vor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein kleiner Python-Exkurs: Der Klasse `Token` müssen die Informationen als einzelne Argumente übergeben werden. Das ist dann ein Problem, wenn die Informationen als Liste in einer einzelnen Variable vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() missing 4 required positional arguments: 'part_of_speech', 'chunk', 'preposition', and 'lemma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-454c823e1975>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'vor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B-PP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B-PNP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vor'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mToken\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __new__() missing 4 required positional arguments: 'part_of_speech', 'chunk', 'preposition', and 'lemma'"
     ]
    }
   ],
   "source": [
    "fields = ['vor', 'IN', 'B-PP', 'B-PNP', 'vor']\n",
    "Token(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Klasse hätte fünf Argumente erwartet, hat aber nur eines, nämlich die Liste mit fünf Elementen, erhalten. (Ein Argument wird immer intern verwendet, daher spricht die Fehlermeldung von 6 und 2 statt von 5 und 1.) Mit `*` lassen sich aber Listen statt einzelner Argumente übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Token(word='vor', part_of_speech='IN', chunk='B-PP', preposition='B-PNP', lemma='vor')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Token(*fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies kann man sich nun zunutze machen, um aus dem »Parse« des Textes eine besser zu verarbeitende Datenstruktur zu gewinnen. Da einzelne Worte durch Leerzeichen und die einzelnen Informationen pro Wort durch Schrägstriche getrennt sind, muss der Parse nur zweimal aufgespalten werden. Die dadurch gewonnenen Einzelinformationen pro Wort werden in der Token-Klasse gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ich/PRP/B-NP/O/ich',\n",
       " 'heiße/VB/B-VP/O/heeißen',\n",
       " 'Sie/PRP/B-NP/O/sie',\n",
       " 'alle/DT/O/O/all',\n",
       " 'ganz/RB/B-ADJP/O/ganz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = parse.split(' ')\n",
    "tokens[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im zweiten Schritt werden die Einzelinformationen der Tokens erneut aufgespalten.\n",
    "\n",
    "Damit immer genau die 5 Informationsfelder aufgespalten werden, aber nicht versehentlich mehr, wird der Parameter `maxsplit` übergeben. Dies ist nur in Randfällen, wie z.B. »2012/13« relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(word='ich', part_of_speech='PRP', chunk='B-NP', preposition='O', lemma='ich'),\n",
       " Token(word='heiße', part_of_speech='VB', chunk='B-VP', preposition='O', lemma='heeißen'),\n",
       " Token(word='Sie', part_of_speech='PRP', chunk='B-NP', preposition='O', lemma='sie'),\n",
       " Token(word='alle', part_of_speech='DT', chunk='O', preposition='O', lemma='all'),\n",
       " Token(word='ganz', part_of_speech='RB', chunk='B-ADJP', preposition='O', lemma='ganz')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [Token(*token.split('/', maxsplit=4)) for token in tokens]\n",
    "tokens[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant wird diese Informationen nun, wenn man sie für die weitere Verarbeitung nutzt. Aufgrund dieser Information kann man die oben angesprochenen Verfahren *Generalisierung* und *Selektion* umsetzen: Als Generalisierung dient hier die Lemmatisierung, die Selektion erfolgt auf Basis der Wortarten.\n",
    "\n",
    "Eine Funktion kann so einen Text nehmen, in Wörter zerlegen, diese auf ihre Grundform zurückführen, und dann nach Wortart filtern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemmatize_and_filter(text, tags):\n",
    "    \"\"\"\n",
    "    Tokenisierung, lemmatisiert und filtert einen Text.\n",
    "    \n",
    "    Der erste Parameter `text` ist dabei ein unverarbeiteter *string*.\n",
    "    \n",
    "    Der zweite Paramter `tags` ist eine Liste von Part-of-Speech-Tags, die in der Ausgabe\n",
    "    berücksichtigt werden sollen. Beispiel: ['NN', 'VB', 'JJ']\n",
    "    \n",
    "    \"\"\"\n",
    "    # Tokenisierung\n",
    "    blob = TextBlob(text, parser=lemma_parser)\n",
    "    parse = blob.parse()\n",
    "    tokens = [Token(*token.split('/', maxsplit=4)) for token in parse.split(' ')]\n",
    "    # Generalisierung und Selektion\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        pos = token.part_of_speech[0:2]\n",
    "        # Filtern\n",
    "        if pos in tags:\n",
    "            if pos == 'NN':\n",
    "                # Substantive immer groß schreiben\n",
    "                result.append(token.lemma.title())\n",
    "            else:\n",
    "                restult.append(token.lemma)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herr -- Kock -- Kollegin -- Frau -- Wanka -- Damen -- Herren -- Preisträgerinnen -- Preisträger -- Forscht -- Bundeskanzleramt -- Jahr -- Jahr -- Preis -- – -- Ihrem -- Leben -- Rolle -- Gespielt -- –\n"
     ]
    }
   ],
   "source": [
    "print(' -- '.join(lemmatize_and_filter(rede, ['NN'])[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Verfahren funktioniert in weiten Teilen, aber alle Methoden der Computerlinguistik sind mit einer gewissen Fehlerrate behaftet. Gute Verfahren haben dabei eine Genauigkeit von über 90% – was in der Gesamtschau immer noch eine Menge Fehler sind. In diesem Fall sieht man, dass der Wortfinde-Algorithmus nicht mit Gedankenstrichen umgehen kann. Hier würde eine entsprechende Vorbereitung des Textes helfen.\n",
    "\n",
    "Ein anderer Ansatz kann sein, nicht über die Wortarten, sondern über eine vorgegebene Liste an Wörtern zu filtern. Je nach Anwendungszweck kann eine solche Stoppwortliste länger oder kürzer sein. Ihr Ziel ist es, nicht bedeutungstragende Wörter herauszufiltern. Dies können neben Partikeln auch Hilfsverben und unspezifische Adjektive und Adverben (z.B. „sehr“) sein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lieb -- Herr -- Kock -- lieb -- Kollegin -- Wanka -- Damen -- Herren -- lieb -- Preisträgerinnen -- Preisträger -- „jugend -- Forscht -- heeißen -- herzlich -- Bundeskanzleramt -- willkomm -- Jahr -- besonder -- Jahr\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('..', 'Daten', 'stopwords.txt')\n",
    "with open(path) as stopwordfile:\n",
    "    stopwords_de = stopwordfile.read().splitlines()\n",
    "\n",
    "from string import punctuation  # !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
    "punctuation += '»«›‹„“”‚‘’–'  # additional quotation marks\n",
    "\n",
    "def lemmatize_and_filter2(text, stopwords):\n",
    "    blob = TextBlob(text, parser=lemma_parser)\n",
    "    parse = blob.parse()\n",
    "    tokens = [Token(*token.split('/', maxsplit=4)) for token in parse.split(' ')]\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        pos = token.part_of_speech[0:2]\n",
    "        word = token.word\n",
    "        if not word.lower() in stopwords and not word.isdigit() and not word in punctuation:\n",
    "            if pos == 'NN':\n",
    "                result.append(token.lemma.title())\n",
    "            else:\n",
    "                result.append(token.lemma)\n",
    "    return result\n",
    "\n",
    "print(' -- '.join(lemmatize_and_filter2(rede, stopwords_de)[0:20]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
