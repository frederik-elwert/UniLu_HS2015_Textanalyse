{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text als Netzwerk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die computerlinguistische Aufbereitung von Texten kann als Ausgangspunkt für weitere Analysen dienen, die typische Muster und Inhalte in Texten sichtbar machen. Eine Grundidee ist dabei, dass Elemente sich nicht allein durch ihre individuellen Eigenschaften auszeichnen, sondern vor allem durch ihre Relationen zu anderen Elementen. In der Soziologie hat dies etwa Mustafa Emirbayer (1997) herausgestellt. Dies gilt aber nicht nur für soziale Akteure, sondern auch für Sprache selbst. So hat etwa bereits Ludwig Wittgenstein in seinen *Philosophischen Untersuchungen* (2008 [1953]) die Verwandtschaft von Begriffen als ein „kompliziertes Netz von Ähnlichkeiten“ (ebd., 57) beschrieben.\n",
    "\n",
    "Diese abstrakten Ideen lassen sich mit den Methoden der Netzwerkanalyse operationalisieren. Ein Netzwerk ist dabei zunächst eine Möglichkeit, Beziehungen zwischen Elementen abzubilden. Die gängigen Attributdaten der Soziologie werden zumeist als Tabelle dargestellt, bei der jede Zeilen einen Fall und jede Spalte dessen Eigenschaften repräsentiert. Im Netzwerk kommt eine zweite Ebene hinzu: Die Beziehungen zwischen diesen Fällen, wobei die einzelnen Verbindungen wieder Eingenschaften haben können (etwa Art, Intensität, Dauer, etc.).\n",
    "\n",
    "In der Sozialforschung werden in der Regel Netzwerke von Akteuren analysiert. Auch solche Netzwerke lassen sich prinzipiell aus Texten gewinnen (etwa Diesner und Carley 2005). Für die Analyse von Texten lassen sich aber auch inhaltliche (semantische) Aspekte als Netzwerk darstellen und analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from textblob_de import TextBlobDE as TextBlob\n",
    "from itertools import combinations\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir hierfür die Verfahren der computerlinguistischen Sprachverarbeitung nutzen können, definieren wir zunächst wieder eine Funktion zur Lemmatisierung und zur Part-of-Speech-Filterung, die die Funktionen von `TextBlob` nutzt.\n",
    "\n",
    "Die Wörter werden dabei nach drei Kriterien gefiltert:\n",
    "\n",
    "* POS-Tags (wobei nur die ersten beiden Buchstaben der Tags berücksichtigt werden, die den Oberkategorien entsprechen, z.B. 'NN' für Substantive),\n",
    "* eine Stopwortliste,\n",
    "* und ein regulärer Ausdruck, der dazu dient, Zahlen enthaltende Wörter (z.B. '1990er', '18jährige') zu entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "with codecs.open('../Daten/stopwords.txt', 'r', 'utf-8') as stopwordfile:\n",
    "    stopwords = [line.strip() for line in stopwordfile.readlines()]\n",
    "\n",
    "def lemmatize_and_filter(blob, tags):\n",
    "    lemmas = blob.words.lemmatize()\n",
    "    tagged = blob.pos_tags\n",
    "    result = []\n",
    "    for lemma, (word, tag) in zip(lemmas, tagged):\n",
    "        if tag[0:2] in tags and not word in stopwords and re.match(u'[^\\W0-9]+', lemma, re.U):\n",
    "            result.append(lemma)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein einfacher Ansatz, Relationen von Texteinheiten (Wörtern) zu bestimmen, ist die Kookkurrenz: Zwei Wörter, die in räumlicher Nähe zueinander stehen, haben eine erhöhte Wahrscheinlichkeit, auch inhaltlich aufeinander bezogen zu sein. Als räumliche Nähe kann z.B. ein Satz definiert werden. Es sind aber auch andere Ansätze möglich, etwa ein fester Wortabstand von etwa 5 Wörtern. Alle (gefilterten) Wörter im jeweiligen Fenster sollen also als vernetzt gelten. Dazu muss für jede Zweierkombination aller Wörter im Fenster eine Relation erzeugt werden.\n",
    "\n",
    "In Python steht dafür die Funktion `combinations()` zur Verfügung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'), ('a', 'c'), ('b', 'c')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(a, b) for a, b in combinations('abc', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Python-Bibliothek [NetworkX](http://networkx.github.io/) stellt einfache Funktionen bereit, ein Netzwerk zu erzeugen. Sie ist dabei allerdings nicht besonders schnell. Für größere Projekte bieten sich andere Bibliotheken wie [igraph](http://igraph.org/python/) an.\n",
    "\n",
    "Das Kernvorgehen ist dabei sehr einfach:\n",
    "\n",
    "1. Das Corpus ist in einer CSV-Datei gespeichert, wobei die Spalte 'text' den eigentlichen Text enhält.\n",
    "2. Für jeden Text wird ein `TextBlob` erzeugt, der linguistische Informationen ergänzt, so u.a. auch Satzgrenzen.\n",
    "3. Für jeden Satz werden die zu berücksichtigenden Lemmata bestimmt. In diesem Fall werden nur Substantive und Adjektive einbezogen.\n",
    "4. Es werden für jeden Satz alle möglichen Kombinationen dieser Lemmata ermittelt.\n",
    "5. Falls eine bestimmte Relation schon im Netzwerk enthalten ist, wird ihr „Gewicht“ um 1 erhöht, andernfalls wird eine neue Relation mit dem Gewicht 1 hinzugefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Daten/reden.csv', parse_dates=['date'], encoding='utf-8')\n",
    "graph = nx.Graph()\n",
    "for text in data['text']:\n",
    "    blob = TextBlob(text)\n",
    "    for sentence in blob.sentences:\n",
    "        lemmas = lemmatize_and_filter(sentence, ('NN', 'JJ'))\n",
    "        for a, b in combinations(lemmas, 2):\n",
    "            if a == b:\n",
    "                continue\n",
    "            try:\n",
    "                graph[a][b]['weight'] += 1\n",
    "            except KeyError:\n",
    "                graph.add_edge(a, b, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Nodes: 46085, Edges: 952228'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u'Nodes: {}, Edges: {}'.format(graph.number_of_nodes(), graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um diese Prozedur nicht für jede Analyse wiederholen zu müssen, kann das Ergebnis in einer Datei gespeichert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.write_graphml(graph, '../Daten/network.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Netzwerk kann nun mit Mitteln der Netzwerkanalyse analysiert werden. Eine einfache Netzwerkstatistik ist der “degree”, die Anzahl der Verbindungen jedes Knotens. `graph.degree()` aus NetworkX gibt dabei in einem `dict` für jeden Knoten seinen degree aus. Die Python-Klasse `Counter` stellt für solche dictionaries einige Zusatzfunktionen bereit, etwa die Ausgabe der `n` Knoten mit dem höchsten degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Deutschland', 7821),\n",
       " (u'gross', 6254),\n",
       " (u'Menschen', 5721),\n",
       " (u'deutsch', 5360),\n",
       " (u'Jahren', 5282),\n",
       " (u'Herren', 5118),\n",
       " (u'Damen', 5087),\n",
       " (u'Deutsch', 4655),\n",
       " (u'Jahr', 4579),\n",
       " (u'Beispiel', 4578),\n",
       " (u'Herr', 4256),\n",
       " (u'Europa', 4239),\n",
       " (u'Welt', 3916),\n",
       " (u'lieb', 3705),\n",
       " (u'Zeit', 3594),\n",
       " (u'Jahre', 3507),\n",
       " (u'Frage', 3494),\n",
       " (u'Land', 3445),\n",
       " (u'europ\\xe4isch', 3409),\n",
       " (u'international', 3396)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "nodes_by_degree = Counter(graph.degree())\n",
    "tops = nodes_by_degree.most_common(20)\n",
    "tops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Gesamtnetzwerk mit seinen 46.000 Knoten ist dabei nicht leicht zu analysieren und inhaltlich zu interpretieren. Es kann aber ein Ausgangspunkt für Detailanalysen sein. Ausgehend von der Annahme, dass sich die Bedeutung eines Wortes durch seinen Verwendungszusammenhang ergibt, lassen sich etwa Begriffsanalysen durchführen. Hierfür kann für ein Wort ein Teilnetzwerk erstellt werden, das nur die mit ihm direkt verbundene Wörter („Nachbarn“) und deren Verbindungen untereinander enthält.\n",
    "\n",
    "Als Beispiel soll das Wort „Integration“ dienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Nodes: 861, Edges: 48052'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph = graph.subgraph(graph.neighbors(u'Integration'))\n",
    "u'Nodes: {}, Edges: {}'.format(subgraph.number_of_nodes(), subgraph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da eher zufällige Verbindungen das Ergebnis verzerren und ja systematische Beziehungen analysiert werden sollen, können die Kanten im Netzwerk noch gefiltert werden. Hier werden alle Kanten, die ein Gewicht von 1 haben, gelöscht. Es können auch höhere Schwellenwerte festgelegt oder aber ausgefeiltere statistische Kriterien zugrundegelegt werden, die die Signifikanz von Verbindungen bestimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24808"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowedges = [edge for edge in subgraph.edges(data=True) if edge[2]['weight'] < 2]\n",
    "len(lowedges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch das Entfernen von Kanten können im Netzwerk „Inseln“ entstehen, die nicht mehr mit dem Gesamtnetzwerk verbunden sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgraph.remove_edges_from(lowedges)\n",
    "nx.number_connected_components(subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese werden für die weitere Analyse ausgeschlossen.\n",
    "\n",
    "*Hinweis:* Hier wird ein Kniff angewandt, der möglichst ressourcensparend arbeitet, aber nicht auf den ersten Blick eingängig ist. Daher hier eine kurze Erläuterung.\n",
    "\n",
    "Die Funktion `connected_component_subgraphs` gibt für jede verbundene Komponente des Netzwerks ein Teilnetzwerk zurück. Da dies aber rechenaufwändig ist und potenziell viel Speicher benötigt, ist das Ergebnis der Funktion keine Liste aller Teilnetzwerke, sondern ein „Generator“. Ein Generator unterscheidet sich von einer Liste dadurch, dass seine Elemente erst „generiert“ werden, wenn auf sie zugegriffen wird. Dabei erlaubt er nur die Form `for x in y`, nicht aber `y[i]`. Ein Generator kann aber in eine Liste umgewandelt werden: `list(y)`.\n",
    "\n",
    "Die Komponenten in `connected_component_subgraphs` sind nach ihrer Größe geordnet, wobei das größte Teilnetzwerk zuerst ausgegeben wird. Um also nur das größte Teilnetzwerk zu erhalten wäre grundsätzlich dieses Vorgehen möglich (und auch nachvollziehbarer):\n",
    "\n",
    "```python\n",
    "subgraphs = list(nx.connected_component_subgraphs(subgraph))\n",
    "subgraph2 = subgraphs[0]\n",
    "```\n",
    "\n",
    "Dafür müssten aber zunächst alle Teilnetzwerke erzeugt und in der Liste `subgraphs` gespeichert werden, obwohl sie nicht benötigt werden. Daher kann hier ein anderer Weg gewählt werden: In der Schleife `for subgraph2 in nx_connected_component_subgraphs(subgraph):` wird im ersten Durchgang das erste Teilnetzwerk erzeugt und der Variable `subgraph2` zugewiesen. Da das alles ist, was wir benötigen, kann die Schleife direkt danach abgebrochen werden, ohne dass weitere Befehle ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Nodes: 739, Edges: 23243'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for subgraph2 in nx.connected_component_subgraphs(subgraph):\n",
    "    break\n",
    "u'Nodes: {}, Edges: {}'.format(subgraph2.number_of_nodes(), subgraph2.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Analyse des Begriffsnetzwerkes von „Integration“ sind diejenigen Bereiche von Interesse, die eine dichtere Vernetzung aufweisen. Diese Teilnetzwerke sind nicht „Komponenten“ im oben genannten Sinne, da sie durchaus Teil des Gesamtnetzwerkes sind. Sie zu identifizieren kann also nur näherungsweise geschehen, es gibt kein absolutes Ergebnis. Dementsprechend sind in den letzten Jahren zahlreiche Algorithmen zur sogenannten “community detection” entwickelt worden, die sich im Detail und in ihrer Effizienz unterscheiden. Ein etablierter Algorithmus für NetworkX ist in [diesem Modul](http://perso.crans.org/aynaud/communities/) verfügbar.\n",
    "\n",
    "*Hinweis:* Die Funktion `best_partition()` gibt für jeden Knoten im Netzwerk die Nummer seiner Community zurück:\n",
    "\n",
    "    {'node1': 0,\n",
    "     'node2': 1,\n",
    "     'node3': 0}\n",
    "\n",
    "Diese Form ist aber nur bedingt geeignet, um die Communities weiter zu analysieren. Besser geeignet wäre eine Liste aller Knoten für jede Community:\n",
    "\n",
    "    [['node1', 'node3'], ['node2']]\n",
    "\n",
    "Daher wird zunächst eine Liste erzeugt, die für jeden möglichen Community-Wert eine leere Liste enthält. Im nächsten Schritt wird jeder Knoten der seiner Community entsprechenden Liste hinzugefügt. Dadurch wird die gewünschte Struktur hergestellt.\n",
    "\n",
    "Im nächsten Schritt kann dann für jede Community ein Teilnetzwerk erzeugt werden. Der Degree der Knoten in den Teilnetzwerken gibt Aufschluss über die zentralen Begriffe dieser Teilnetzwerke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 0:\n",
      "[(u'Europa', 124),\n",
      " (u'europ\\xe4isch', 116),\n",
      " (u'Europ\\xe4isch', 107),\n",
      " (u'Welt', 106),\n",
      " (u'Beispiel', 99),\n",
      " (u'gemeinsam', 98),\n",
      " (u'Union', 97),\n",
      " (u'L\\xe4nder', 96),\n",
      " (u'L\\xe4ndern', 87),\n",
      " (u'Weg', 81)]\n",
      "Community 1:\n",
      "[(u'Menschen', 146),\n",
      " (u'Frage', 119),\n",
      " (u'Zukunft', 116),\n",
      " (u'Gesellschaft', 113),\n",
      " (u'Politik', 94),\n",
      " (u'Thema', 91),\n",
      " (u'Leben', 89),\n",
      " (u'Aufgabe', 88),\n",
      " (u'politisch', 88),\n",
      " (u'wichtig', 86)]\n",
      "Community 2:\n",
      "[(u'Deutschland', 147),\n",
      " (u'deutsch', 114),\n",
      " (u'gross', 106),\n",
      " (u'Jahren', 96),\n",
      " (u'Land', 90),\n",
      " (u'Jahre', 77),\n",
      " (u'Zeit', 77),\n",
      " (u'international', 77),\n",
      " (u'Geschichte', 76),\n",
      " (u'Seite', 66)]\n",
      "Community 3:\n",
      "[(u'Herr', 91),\n",
      " (u'lieb', 89),\n",
      " (u'Herren', 78),\n",
      " (u'Damen', 78),\n",
      " (u'Deutsch', 75),\n",
      " (u'Frau', 67),\n",
      " (u'herzlich', 57),\n",
      " (u'Professor', 50),\n",
      " (u'freue', 46),\n",
      " (u'Berlin', 43)]\n",
      "Community 4:\n",
      "[(u'Jahr', 24),\n",
      " (u'Bundesregierung', 22),\n",
      " (u'Euro', 20),\n",
      " (u'Rahmen', 17),\n",
      " (u'Bildung', 17),\n",
      " (u'Millionen', 15),\n",
      " (u'Bereich', 15),\n",
      " (u'kulturell', 15),\n",
      " (u'zus\\xe4tzlich', 15),\n",
      " (u'Milliarde', 13)]\n"
     ]
    }
   ],
   "source": [
    "import community\n",
    "partition = community.best_partition(subgraph2)\n",
    "\n",
    "communities = [[] for i in set(partition.values())]\n",
    "for node, comm in partition.items():\n",
    "    communities[comm].append(node)\n",
    "    \n",
    "community_graphs = [subgraph2.subgraph(nodes) for nodes in communities]\n",
    "for i, graph in enumerate(community_graphs):\n",
    "    print u'Community {}:'.format(i)\n",
    "    pprint(Counter(graph.degree()).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Ausgabe gibt einen ersten Anhaltspunkt für die Interpretation des Begriffs „Integration“:\n",
    "\n",
    "1. Die erste Community umfasst den Bereich der europäischen Integration.\n",
    "2. Die zweite Community verweist auf Integration als gesellschaftliche Aufgabe und Zukunftsfrage.\n",
    "3. Die dritte Community bezieht sich auf die deutsche Geschichte, ist aber zunächst nicht weiter inhaltlich bestimmt. Hier könnte noch eine detailliertere Analyse vorgenommen werden.\n",
    "4. Die vierte Community ist ein Artefakt, das sich aus den in allen Reden enthaltenen Anreden ergibt. Dies könnte ein Hinweis darauf sein, die Anreden vor der Analyse aus den Texten zu entfernen.\n",
    "5. Die fünfte Community bestimmt Integration in Hinblick auf politische Investitionen in Bildung und Kultur.\n",
    "\n",
    "Dies gibt einen ersten Eindruck davon, in welchen Kontexten Integration in politischen Reden der aktuellen deutschen Bundesregierung thematisiert wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literatur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diesner, Jana und Kathleen M. Carley (2005): „Revealing social structure from texts: meta-matrix text analysis as a novel method for network text analysis“, Causal mapping for information systems and technology research: Approaches, advances, and illustrations , S. 81–108, http://andrew.cmu.edu/user/jdiesner/publications/DiesnerCarley_meta_matrix_text_analysis.pdf (zugegriffen am 8.9.2013).\n",
    "\n",
    "Emirbayer, Mustafa (1997): „Manifesto for a relational sociology“, American Journal of Sociology 103/2, S. 281–317, http://www.jstor.org/stable/10.1086/231209 (zugegriffen am 29.10.2013).\n",
    "\n",
    "Wittgenstein, Ludwig (2008): Philosophische Untersuchungen, hg. von. Joachim Schulte, Bibliothek Suhrkamp 1372, Frankfurt am Main: Suhrkamp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
